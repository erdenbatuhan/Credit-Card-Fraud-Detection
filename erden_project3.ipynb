{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-color: black; border-style: groove;\">\n",
    "<center><h3> CS 452/552 Data Science with Python </h3></center>\n",
    "<center><h3> Project 3 </h3></center>\n",
    "<center><h3> Classification Challange - Credit Card Fraud Detection </h3></center>\n",
    "<center><h3> Batuhan Erden </h3></center>\n",
    "<hr style=\"border-color: black; border-style: groove;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Imports </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import svm, linear_model, metrics, preprocessing, model_selection, neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Global Constants </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Read dataset from csv </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Fraud  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(DATASET_PATH + \"creditcard.csv\")\n",
    "dataset = dataset.rename(columns={\"Class\": \"Fraud\"})  # Rename feature 'Class' to 'Fraud'\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Plot to see how unbalanced the data is </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492 frauds out of 284807 transactions which is 0.1727% percent of all transactions..\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEICAYAAACJalkVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0jnZmJlZ6f4Hxl+iqxLKTsYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1189081d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate\n",
    "fraud_column_counts = dataset.Fraud.value_counts()\n",
    "\n",
    "num_fraud = fraud_column_counts[1]\n",
    "num_transactions = sum(fraud_column_counts)\n",
    "\n",
    "print(\"{} frauds out of {} transactions which is {:.4f}% percent of all transactions..\".\n",
    "      format(num_fraud, num_transactions, num_fraud / num_transactions * 100))\n",
    "\n",
    "# Plot\n",
    "plt.title(\"Histogram to show that the data is unbalanced\")\n",
    "plt.ylabel('Number of transactions')\n",
    "\n",
    "plt.xticks([0, 1], [\"Normal\", \"Fraud\"])\n",
    "plt.bar([0, 1], [num_transactions - num_fraud, num_fraud], align=\"center\", alpha=0.5)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Ignore features that have almost no effect on the frauds </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features to be ignored: ['Time', 'V8', 'V13', 'V15', 'V20', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V14</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V21</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V9       V10       V11       V12       V14       V16       V17  \\\n",
       "0  0.363787  0.090794 -0.551600 -0.617801 -0.311169 -0.470401  0.207971   \n",
       "1 -0.255425 -0.166974  1.612727  1.065235 -0.143772  0.463917 -0.114805   \n",
       "2 -1.514654  0.207643  0.624501  0.066084 -0.165946 -2.890083  1.109969   \n",
       "3 -1.387024 -0.054952 -0.226487  0.178228 -0.287924 -1.059647 -0.684093   \n",
       "4  0.817739  0.753074 -0.822843  0.538196 -1.119670 -0.451449 -0.237033   \n",
       "\n",
       "        V18       V19       V21  Amount  Fraud  \n",
       "0  0.025791  0.403993 -0.018307  149.62      0  \n",
       "1 -0.183361 -0.145783 -0.225775    2.69      0  \n",
       "2 -0.121359 -2.261857  0.247998  378.66      0  \n",
       "3  1.965775 -1.232622 -0.108300  123.50      0  \n",
       "4 -0.038195  0.803487 -0.009431   69.99      0  "
      ]
     },
     "execution_count": 830,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_threshold = 0.6\n",
    "\n",
    "v_features = dataset.iloc[:, 1:29].columns\n",
    "v_features_to_drop = [\"Time\"]\n",
    "\n",
    "for v_feature in v_features:\n",
    "    mean_fraud = np.mean(dataset[v_feature][dataset[\"Fraud\"] == 1])\n",
    "    \n",
    "    if -mean_threshold <= mean_fraud <= mean_threshold:\n",
    "        v_features_to_drop.append(v_feature)\n",
    "\n",
    "dataset = dataset.drop(v_features_to_drop, axis=1)\n",
    "\n",
    "print(\"Features to be ignored: %a\" % v_features_to_drop)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Normalize amounts </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Amount\n",
       "0  0.244964\n",
       "1 -0.342475\n",
       "2  1.160686\n",
       "3  0.140534\n",
       "4 -0.073403"
      ]
     },
     "execution_count": 831,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"Amount\"] = preprocessing.StandardScaler().fit_transform(dataset[\"Amount\"].values.reshape(-1, 1))\n",
    "dataset.iloc[:, dataset.columns == \"Amount\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Minimize and shuffle the dataset </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safe == Fraudulent? --> True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V14</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V21</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>214662</th>\n",
       "      <td>0.467992</td>\n",
       "      <td>1.100118</td>\n",
       "      <td>-5.607145</td>\n",
       "      <td>2.204714</td>\n",
       "      <td>-0.578539</td>\n",
       "      <td>-0.174200</td>\n",
       "      <td>-3.454201</td>\n",
       "      <td>-1.065016</td>\n",
       "      <td>-5.416037</td>\n",
       "      <td>4.497929</td>\n",
       "      <td>-5.019610</td>\n",
       "      <td>-7.914989</td>\n",
       "      <td>-4.472014</td>\n",
       "      <td>-5.856998</td>\n",
       "      <td>-2.243178</td>\n",
       "      <td>-0.173814</td>\n",
       "      <td>0.983481</td>\n",
       "      <td>0.128700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80427</th>\n",
       "      <td>-1.519120</td>\n",
       "      <td>3.424339</td>\n",
       "      <td>-3.540469</td>\n",
       "      <td>1.669922</td>\n",
       "      <td>-0.448989</td>\n",
       "      <td>-1.676307</td>\n",
       "      <td>-0.807759</td>\n",
       "      <td>-0.471705</td>\n",
       "      <td>-1.147762</td>\n",
       "      <td>0.771071</td>\n",
       "      <td>0.494893</td>\n",
       "      <td>-2.480540</td>\n",
       "      <td>0.861134</td>\n",
       "      <td>3.909449</td>\n",
       "      <td>1.026224</td>\n",
       "      <td>-1.022979</td>\n",
       "      <td>0.749636</td>\n",
       "      <td>-0.323004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157585</th>\n",
       "      <td>1.934946</td>\n",
       "      <td>0.650678</td>\n",
       "      <td>-0.286957</td>\n",
       "      <td>3.987828</td>\n",
       "      <td>0.316052</td>\n",
       "      <td>-0.099449</td>\n",
       "      <td>-0.021483</td>\n",
       "      <td>0.508730</td>\n",
       "      <td>1.072955</td>\n",
       "      <td>-0.427567</td>\n",
       "      <td>-2.777649</td>\n",
       "      <td>1.571080</td>\n",
       "      <td>0.908650</td>\n",
       "      <td>-0.122016</td>\n",
       "      <td>-0.104110</td>\n",
       "      <td>-1.684022</td>\n",
       "      <td>-0.173602</td>\n",
       "      <td>-0.349231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150662</th>\n",
       "      <td>-5.839192</td>\n",
       "      <td>7.151532</td>\n",
       "      <td>-12.816760</td>\n",
       "      <td>7.031115</td>\n",
       "      <td>-9.651272</td>\n",
       "      <td>-2.938427</td>\n",
       "      <td>-11.543207</td>\n",
       "      <td>-3.494276</td>\n",
       "      <td>-13.320789</td>\n",
       "      <td>8.460244</td>\n",
       "      <td>-17.003289</td>\n",
       "      <td>-14.094452</td>\n",
       "      <td>-12.661696</td>\n",
       "      <td>-18.912494</td>\n",
       "      <td>-6.626975</td>\n",
       "      <td>4.008921</td>\n",
       "      <td>2.462056</td>\n",
       "      <td>0.910406</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150687</th>\n",
       "      <td>-10.300820</td>\n",
       "      <td>6.483095</td>\n",
       "      <td>-15.076363</td>\n",
       "      <td>6.554191</td>\n",
       "      <td>-8.880252</td>\n",
       "      <td>-4.471672</td>\n",
       "      <td>-14.900689</td>\n",
       "      <td>-4.358441</td>\n",
       "      <td>-14.533162</td>\n",
       "      <td>7.588741</td>\n",
       "      <td>-15.835719</td>\n",
       "      <td>-11.567006</td>\n",
       "      <td>-11.467430</td>\n",
       "      <td>-19.172996</td>\n",
       "      <td>-6.969856</td>\n",
       "      <td>2.211756</td>\n",
       "      <td>1.508748</td>\n",
       "      <td>-0.349231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               V1        V2         V3        V4        V5        V6  \\\n",
       "214662   0.467992  1.100118  -5.607145  2.204714 -0.578539 -0.174200   \n",
       "80427   -1.519120  3.424339  -3.540469  1.669922 -0.448989 -1.676307   \n",
       "157585   1.934946  0.650678  -0.286957  3.987828  0.316052 -0.099449   \n",
       "150662  -5.839192  7.151532 -12.816760  7.031115 -9.651272 -2.938427   \n",
       "150687 -10.300820  6.483095 -15.076363  6.554191 -8.880252 -4.471672   \n",
       "\n",
       "               V7        V9        V10       V11        V12        V14  \\\n",
       "214662  -3.454201 -1.065016  -5.416037  4.497929  -5.019610  -7.914989   \n",
       "80427   -0.807759 -0.471705  -1.147762  0.771071   0.494893  -2.480540   \n",
       "157585  -0.021483  0.508730   1.072955 -0.427567  -2.777649   1.571080   \n",
       "150662 -11.543207 -3.494276 -13.320789  8.460244 -17.003289 -14.094452   \n",
       "150687 -14.900689 -4.358441 -14.533162  7.588741 -15.835719 -11.567006   \n",
       "\n",
       "              V16        V17       V18       V19       V21    Amount  Fraud  \n",
       "214662  -4.472014  -5.856998 -2.243178 -0.173814  0.983481  0.128700      1  \n",
       "80427    0.861134   3.909449  1.026224 -1.022979  0.749636 -0.323004      0  \n",
       "157585   0.908650  -0.122016 -0.104110 -1.684022 -0.173602 -0.349231      1  \n",
       "150662 -12.661696 -18.912494 -6.626975  4.008921  2.462056  0.910406      1  \n",
       "150687 -11.467430 -19.172996 -6.969856  2.211756  1.508748 -0.349231      1  "
      ]
     },
     "execution_count": 864,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_transactions = dataset[dataset[\"Fraud\"] == 0]\n",
    "fraudulent_transactions = dataset[dataset[\"Fraud\"] == 1]\n",
    "\n",
    "random_safe_transactions = safe_transactions.sample(n=len(fraudulent_transactions))\n",
    "\n",
    "data_minimized = pd.concat(([fraudulent_transactions, random_safe_transactions]), axis=0)\n",
    "data_minimized = data_minimized.sample(frac=1)  # Shuffle the dataset\n",
    "\n",
    "print(\"Safe == Fraudulent? --> {}\".format(len(data_minimized) / 2 == len(fraudulent_transactions)))\n",
    "data_minimized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Prepare inputs and labels </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs_and_labels(data):\n",
    "    inputs = data.iloc[:, data.columns != \"Fraud\"]\n",
    "    labels = data.iloc[:, data.columns == \"Fraud\"]\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    labels = np.array(labels).flatten()\n",
    "    \n",
    "    return inputs, labels\n",
    "    \n",
    "mini_X, mini_y = get_inputs_and_labels(data_minimized)\n",
    "X, y = get_inputs_and_labels(dataset)\n",
    "\n",
    "mini_X_train, mini_X_test, mini_y_train, mini_y_test = model_selection.train_test_split(\n",
    "    mini_X, mini_y, test_size=.3, random_state=0)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test the performance of the dataset using K-Fold Cross Validation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_using_kfold(n_splits, classifier, X, y):\n",
    "    kfold = model_selection.KFold(n_splits=5, shuffle=False)\n",
    "    scores = []\n",
    "\n",
    "    # Test\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        classifier.fit(X[train_index], y[train_index])\n",
    "        score = classifier.score(X[test_index], y[test_index])\n",
    "\n",
    "        scores.append(score)\n",
    "        print(\"....\", end=\"....\")\n",
    "    \n",
    "    # Print the average score\n",
    "    print(\"\\n- Average Score of {:d}-Fold Cross Validation: {:.2f}%\".format(n_splits, np.mean(scores) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Calculate Confusion Matrix </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(classifier, test, predictions):\n",
    "    np.set_printoptions(precision=4)\n",
    "    confusion_matrix = metrics.confusion_matrix(test, predictions)\n",
    "    \n",
    "    # Print Confusion Matrix\n",
    "    print(\"- Confusion Matrix:\")\n",
    "    print(confusion_matrix)\n",
    "    \n",
    "    # Print Normalized Confusion Matrix\n",
    "    confusion_matrix_normalized = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    print(\"- Normalized Confusion Matrix:\")\n",
    "    print(confusion_matrix_normalized)\n",
    "    \n",
    "    # Print the prediction accuracy of fraudulent transactions\n",
    "    print(\"- Prediction accuracy of fraudulent transactions (Recall Accuracy): {:.2f}%\"\n",
    "          .format(confusion_matrix_normalized[1, 1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Classifier 1: LogisticRegression </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li> Test the mini data in which # of safe transactions and # of fraudulent transactions is equal (The data is balanced) </li>\n",
    "<li> Test for both L1 & L2 Regularization </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression (L1 Regularization)\n",
      "........................................\n",
      "- Average Score of 5-Fold Cross Validation: 76.63%\n",
      "- Confusion Matrix:\n",
      "[[ 89  62]\n",
      " [  9 136]]\n",
      "- Normalized Confusion Matrix:\n",
      "[[ 0.5894  0.4106]\n",
      " [ 0.0621  0.9379]]\n",
      "- Prediction accuracy of fraudulent transactions (Recall Accuracy): 93.79%\n",
      "\n",
      "LogisticRegression (L2 Regularization)\n",
      "........................................\n",
      "- Average Score of 5-Fold Cross Validation: 89.33%\n",
      "- Confusion Matrix:\n",
      "[[127  24]\n",
      " [  8 137]]\n",
      "- Normalized Confusion Matrix:\n",
      "[[ 0.8411  0.1589]\n",
      " [ 0.0552  0.9448]]\n",
      "- Prediction accuracy of fraudulent transactions (Recall Accuracy): 94.48%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for regularization in zip([\"L1\", \"L2\"], [\"Regularization\"] * 2):\n",
    "    print(\"LogisticRegression (%s %s)\" % (regularization[0], regularization[1]))\n",
    "    \n",
    "    classifier = linear_model.LogisticRegression(C=.001, penalty=regularization[0].lower())\n",
    "    test_using_kfold(5, classifier, mini_X, mini_y)\n",
    "    \n",
    "    classifier.fit(mini_X_train, mini_y_train)\n",
    "    mini_predictions = classifier.predict(mini_X_test)\n",
    "    \n",
    "    compute_confusion_matrix(classifier, mini_y_test, mini_predictions)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li> Test the actual data (The data is unbalanced) </li>\n",
    "<li> Test for both L1 & L2 Regularization </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression (L1 Regularization)\n",
      "........................................\n",
      "- Average Score of 5-Fold Cross Validation: 99.85%\n",
      "- Confusion Matrix:\n",
      "[[48879 36417]\n",
      " [    7   140]]\n",
      "- Normalized Confusion Matrix:\n",
      "[[ 0.5731  0.4269]\n",
      " [ 0.0476  0.9524]]\n",
      "- Prediction accuracy of fraudulent transactions (Recall Accuracy): 95.24%\n",
      "\n",
      "LogisticRegression (L2 Regularization)\n",
      "........................................\n",
      "- Average Score of 5-Fold Cross Validation: 99.91%\n",
      "- Confusion Matrix:\n",
      "[[71057 14239]\n",
      " [    8   139]]\n",
      "- Normalized Confusion Matrix:\n",
      "[[ 0.8331  0.1669]\n",
      " [ 0.0544  0.9456]]\n",
      "- Prediction accuracy of fraudulent transactions (Recall Accuracy): 94.56%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for regularization in zip([\"L1\", \"L2\"], [\"Regularization\"] * 2):\n",
    "    print(\"LogisticRegression (%s %s)\" % (regularization[0], regularization[1]))\n",
    "    \n",
    "    classifier = linear_model.LogisticRegression(C=.001, penalty=regularization[0].lower())\n",
    "    test_using_kfold(5, classifier, X, y)\n",
    "\n",
    "    classifier.fit(mini_X_train, mini_y_train)\n",
    "    predictions = classifier.predict(X_test)\n",
    "\n",
    "    compute_confusion_matrix(classifier, y_test, predictions)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Classifier 2: LinearSVC </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li> Test the mini data in which # of safe transactions and # of fraudulent transactions is equal (The data is balanced) </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC\n",
      "........................................\n",
      "- Average Score: 94.00%\n",
      "- Confusion Matrix:\n",
      "[[145   6]\n",
      " [ 14 131]]\n",
      "- Normalized Confusion Matrix:\n",
      "[[ 0.9603  0.0397]\n",
      " [ 0.0966  0.9034]]\n",
      "- Prediction accuracy of fraudulent transactions: 90.34%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"LinearSVC\")\n",
    "\n",
    "classifier = svm.LinearSVC()\n",
    "test_using_kfold(5, classifier, mini_X, mini_y)\n",
    "\n",
    "classifier.fit(mini_X_train, mini_y_train)\n",
    "mini_predictions = classifier.predict(mini_X_test)\n",
    "\n",
    "compute_confusion_matrix(classifier, mini_y_test, mini_predictions)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li> Test the actual data (The data is unbalanced) </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC\n",
      "........................................\n",
      "- Average Score: 99.91%\n",
      "- Confusion Matrix:\n",
      "[[82780  2516]\n",
      " [   13   134]]\n",
      "- Normalized Confusion Matrix:\n",
      "[[ 0.9705  0.0295]\n",
      " [ 0.0884  0.9116]]\n",
      "- Prediction accuracy of fraudulent transactions: 91.16%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"LinearSVC\")\n",
    "\n",
    "classifier = svm.LinearSVC()\n",
    "test_using_kfold(5, classifier, X, y)\n",
    "\n",
    "classifier.fit(mini_X_train, mini_y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "compute_confusion_matrix(classifier, y_test, predictions)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
